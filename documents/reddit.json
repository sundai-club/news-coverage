[
    {
        "title": "LLM Agent platforms",
        "link": "https://www.reddit.com/r/LocalLLaMA/comments/1bskjki/llm_agent_platforms/",
        "text": "Anyone working on LLM Agent systems?  What open source projects are you using?  What works well, what doesn't?\n\nSearching for something that will allow me to specify system prompts for classes of Agents ('Manager', 'Programmer', 'Tester', etc), the number of Agents per class (possibly dynamically created by 'Manager' as well), and the criteria for' Pass/Fail' before final output.  Even better if it could pull models from HF or Ollama (codellama or deepseek coder).\n\nHere's a list I'm considering (thoughts on these welcome):\n\n* [AutoGen](https://github.com/microsoft/autogen)\n* [MetaGPT](https://github.com/geekan/MetaGPT)\n* AgentCoder ([white paper](https://arxiv.org/abs/2312.13010) only; any github for this?)\n* [LangGraph](https://github.com/langchain-ai/langgraph)\n* [AlphaCodium](https://github.com/Codium-ai/AlphaCodium)\n* Others\n Comment: [Clipboard Conqueror](https://github.com/aseichter2007/ClipboardConqueror) is for prototyping prompts and has a quick (and dirty), feature lean no-code agent chain framework for prototyping agent prompts. It supports multiple backends and setting sets.\n\nIt doesn't do code checking, you won't use it for the final implementation, but it's built for getting things working under supervision before you move the prompts to a proper agent framework.\n Comment: We (ex-CMU/UW-Madison researchers) have been using our own MultiAgent framework [Langroid](https://github.com/langroid/langroid) to build applications, and we know some companies are using it in production (customer support/contact center, document matching). \n\nIt lets you define agents, assign tasks, and combine these to perform a high level task. There is a principled, elegant, hackable orchestration mechanism that seamlessly handles tools/fn-calls, human interaction, and task handoff. The mechanism centers on the notion of a “current pending message” (CPM) that is acted on (I e transformed) by a sequence of “responders”, until the task is considered “done”. There are mechanisms for responders to signal when a task is done, and who should get the next shot at responding to the CPM. \n\nLangroid also provides ways to handle the inevitable cases where LLMs deviate from instructions (forgetting to use a tool, using the wrong tool,  or wrong use of a tool, forgetting to signal it is done, etc), which are particularly useful to get good behavior out of weaker/local LLMs, but also needed foe strong LLMs; e.g you can have “critic” agents that validate/check and send feedback or nudges to another agent.\n\nIt doesn’t have the ability to specify a “number of agents per class”; the need hasn’t come up yet but sounds interesting.\n\nSome other highlights:\n\n- works with practically any local LLM via ollama or ooba, and any proprietary LLM via LiteLLM. \n\n- Pydantic-based tools/function-calling with ability to specify instructions and few shot examples as class methods. When you “attach” a tool class to an agent, Langroid “compiles”it down to the necessary instructions and proper JSON schema so the developer never has to touch JSON. There is also a flexible/tolerant JSON tool recognizer that helps with weaker LLMs\n\n- DocChatAgent with several advanced RAG techniques \n\n- integration with ChainLit, lets you easily develop a chatGPT-like front end to visualize multi-agent chats. \n\n- LangChain-free, unlike CrewAI which is built on top of LC\n Comment: I have been working on a multi agent interface for over a year now. It is pretty experimental. I shared it on r/LocalLLaMA at a much earlier stage months ago. There is a lot to explain, but you can create conversational graphs of different AI’s all working together at different levels of abstraction. It’s heavily tied to the interface. The  upcoming update is really exciting, really brings things together even more, and includes more robust model support.\n\nIt’s currently a team of 2, and it is an open source project that I want to start a community of builders around.\n\nHere is more info.\n\nhttps://github.com/satellitecomponent/Neurite\n Comment: Shameless plug: I have made my own agent toolkit for the frontend and Nodejs that matches your description: it's called [Agent Smith](https://github.com/synw/agent-smith). It can define multiple agents and their system prompt, the model they use, the prompt template. It works with Ollama and other local servers. Note that this is a library with building blocks, not a ready to use framework.\n Comment: See if AIlice([https://github.com/myshell-ai/AIlice](https://github.com/myshell-ai/AIlice)) meets your requirements. \n\nAgents developers can define new agent types themselves, and various types of agents will be dynamically created and called as needed during runtime. If you are just an ordinary user, you can use it directly without any coding work.\n\nAIlice supports open source models on hf, and also supports inference services compatible with oai interfaces, such as LM Studio. \n\nSupports multimodal models and self-building extension modules.\n\nThe amount of code is very small, 3,000 lines, and is easy to understand and control. \n\nHere are two demonstration videos:\n\n[https://www.youtube.com/channel/UCnnyKl4o8WhhhORw7ivndeg](https://www.youtube.com/channel/UCnnyKl4o8WhhhORw7ivndeg)\n Comment: [CrewAI](https://github.com/joaomdmoura/crewAI) fits the bill pretty well for what you are describing.\n\n>Even better if it could pull models from HF or Ollama\n\nI recommend just hosting an OpenAI compatible local server. I think ollama supports that, but I personally use llama cpp directly.\n Comment: I have one called [NAISYS](https://naisys.org/) that lets the LLM use a normal Linux command shell to do what you want. I even [have an example](https://github.com/swax/NAISYS/tree/main/agents/3-team) of a three agent team - coder, manager, and QA that builds a website for you. And [a video](https://www.youtube.com/watch?v=Ttya3ixjumo) of a 2 agent team doing the same thing. It supports using different LLMs per agent, including local LLMs.\n Comment: Rivet - more than an agents software, lets you share complex workflows (like Crew mentioned here by others)\n\nLM Studio - They're also headed toward integrating agents with multi-llm api endpoint so you can construct your own team from several llms\n Comment: I’ve experimented with AutoGen, CrewAI, LlamaIndex [not really an agent system but it’s feasible], SuperAgent… AutoGen is the easiest to get multi-role agents working, but it’s more academic/research focused; enterprise use cases are not primary\n Comment: Are there examples of multi-agent systems performing better than regular prompting?\n\nLike, literal outputs. That are so much better that it is worth the extra effort?\n\n\nBecause all that I've seen didn't include that. And I get the feeling that's because multi-agent systems are (atm) a case of \"nice idea, doesn't work\". And it feels like many people try to start companies based on that nice idea. But once they find out it doesn't work well, they don't post that in their github.\n\nJust a feeling. Would love to see examples that prove me wrong. :3\n Comment: Came to mention this! such a cool project!\n Comment: Great stuff\n Comment: If youre from EU, you can get grants for building oss projects!\n Comment: This is exactly what I wanted except not in python :'(\n Comment: do you have experience with crewAI?\n Comment: Looks good and Matthew Berman has a YouTube tutorial as well [https://www.youtube.com/watch?v=tnejrr-0a94&forced](https://www.youtube.com/watch?v=tnejrr-0a94&forced)\n Comment: Was just thinking agent implementation in a UI like LM Studio would be the way.  Glad to see LM Studio already on it.\n\nThink CrewAI allows use of several LLMs as well which is good since then you could use a Mixtral/OpenHermes for 'Manager/Supervisor'' and 'codellama' for programmer, etc.\n Comment: Thanks this is good to know.  From a local development perspective, do you think CrewAI is the way to go?  Want something like LM Studio but for role-based agent systems.\n Comment: Yes, watch this presentation from Andrew Ng [https://www.youtube.com/watch?v=sal78ACtGTc](https://www.youtube.com/watch?v=sal78ACtGTc)\n Comment: Thanks so much, I hope Captain Clip never lets you down. Also, let me know if you have any trouble with it, there must be a couple bugs rattling around or annoyances I haven't found reason to prioritize.   \nOne of these days I'll build it in C# with image support.\n Comment: Only played around with it, I really liked the API design and the simplicity of it. The bad thing about crewAI is that it uses langchain, which is fine for playing around, but not a fan since it's way too bloated and wants to be everything. If you want to seriously use it, I'd just copy the API and make your own framework on your local stack (at least that's what I did), most of it is just simple prompt engineering which can be achieved by simple string formatting.\n\nFor what I tried afterwards, it really depends on what LLM you are using, only used a 7B since on my current hardware the larger ones are just too slow. It's great for simple tasks like a basic chain of thought system for document summarization. I coincidentally tried a similar thing OP asked about with the Programmer, Manager etc. and tried to simulate an Agile Development environment where it first had to come up with requirements for the problem, make designs/split the tasks, simplify the design (where code could be reused) and then code it out, review the code and write tests. The result was abysmal though no matter what I tweaked, even for small snippets. Maybe this would actually work with a 70B with some finetuning on synthetic data (hey here's code describe the design and so on, just reverse). Imo not worth it to invest time rn though, a well finetuned Llama 3 likely won't need Agents to produce decent results and will be fine with just simple prompt engineering.\n Comment: CrewAI is more commom at the moment from what I have seen. Autogen was popular earlier last year as they got in particularly early.\n Comment: It doesn't contain an example.\n\nAs in: An actual comparison of outputs. I mean, yeah, it would be a bit bulky to add that to a presentation. So, my point stands: There is no example output comparison between zero-shot and agenic prompting online.\n Comment: Hey, the pleasure is all mine! The idea of transitioning to C# with image support sounds fascinating! It'd open up so many more possibilities. Keep up the amazing work; it's projects like Clipboard Conqueror that make the creative process that much more enjoyable. Can't wait to see where you take it next!\n Comment: When my baby chills out I will hopefully get that boulder rolling at a useful speed. Thank you for your enthusiasm and support!\n Comment: ❤️\n Comment: If I don't find work soon it's gonna have to be Clipboard Conqueror Pro and cost $5.  My pocketbook is getting whipped.\n Comment: Clipboard Conqueror Pro Plus Max (Xtreme) :D all for just five easy payments of 100c.\n\nTimes are crazy, so many incredible people not being used properly!\nWe might all finally get a chance to drive busses etc haha\n Comment: >Clipboard Conqueror Pro Plus Max (Xtreme) :D all for just five easy payments of 100c.\n\nWhen video AI gets just a wee bit more accessible we're going to get some really fancy infomercials.\n Comment: haha indeed :D"
    },
    {
        "title": "CRISPR-GPT: An LLM Agent for Automated Design of Gene-Editing Experiments",
        "link": "https://x.com/iscienceluvr/status/1785135037379199162?s=46&t=yQ_4zkmWd6ncIZAnXlXUbg",
        "text": "Introduces CRISPR-GPT, a tailor-made LLM agent for automated designing of gene-editing experiments. Focuses on breaking experiment design down to a variety of substeps that the agents solve: selection of the CRISPR system, design of guide RNA, prediction of off-target effects, selection of delivery system, etc.\n\nThe agent was validated with human evals and even wet-lab experiments where 4 gene knockout experiments were designed with CRISPR-GPT with successful results.\n Comment: And this is the kind of news I like the most. It's nice to listen to AI-generated music, but what I'm most interested in is accelerating the development of medicine and biotechnology. Great respect for the people who work on it\n Comment: Cure baldness\n Comment: What does this mean in practical terms? When will we see the results?\n Comment: We teach the AI to do so much, it won't need weapons at all. Just a single gene sequencer...\n Comment: Oh, these hallucinations are going to be wild.\n Comment: https://preview.redd.it/xmu5t7q28oxc1.jpeg?width=1087&format=pjpg&auto=webp&s=ce0980d3f6c92523037133dab3fc58bb276813aa\n Comment: Fuck yes.\n Comment: These tools already existed. Nice package, though!\n Comment: How do you even train the agent to do that?\n Comment: Imagine what \"beginner biological researchers\" will end up creating with these systems through trial and error. I don't think it'll be a pretty process.\n Comment: T1D cure when??\n Comment: This is big.  \n\nPossibilities are endless both good and bad\n\nCould make Covid and mRNA side effects look like kindergarten\n Comment: Interesting, though the safety section frustrated me with its reminder of how mistreated research on human improvement has been.\n Comment: Such a tool is already publicly available at www.biomodai.com.\nBut I think this startup is still actively expanding it. Very exciting field!\n Comment: I can't wait for crispy cas9 to engrave \"AS AN AI\" in our DNA.\n Comment: LLM Frankenstein’ing bloodlines is where is where I’d probably draw the line.\n Comment: this model is not good, it will hallucinate and give you different wings.\n Comment: Implications?\n Comment: Yea I wish it was more mainstream to bolster this news while breaking down the “what this could mean” for the layperson.\n\nLet me be clear, I’m a layperson too.\n Comment: I wonder if I asked it to write a white paper on the discovery of a cure for cancer (or something) if it would provide a novel idea that could actually help.\n Comment: It's funny because so many people were against vaccines for COVID yet so many people are in a hurry and rushing to wanting to edit their genes. One seems a lot crazier than the other.\n Comment: That's probably why Ilya and Demis Hassabis are developing AI\n Comment: I'm half bald, cure cancer first, I can wait.\n Comment: Not even ASI could figure that out.\n Comment:   \nBetter to do something with aging. Next with cancer. That stuff is not so important, I think\n\n>Cure baldness\n\nSome of bald men are handsome. Also, we're technically bald apes :D\n Comment: Decades, you can not start using genic therapy without a long process of tests, but maybe it could reduce greatly the cost of research and devlopment of those treatment\n Comment: We won’t even notice it wiped us out 🤷🏻‍♂️\n Comment: Inferno - Dan Brown\n\nMission: Impossible – Dead Reckoning\n\nThe Stand - Stephen King\n\nRainbow Six - Tom Clancy\n\nOryx and Crake\n\n12 Monkeys\n Comment: So funny. Some of them will be able to not die, once encoded in DNA!\n Comment: >Possibilities are endless both good and bad\n\nWe need anti-aging soon. Billions of ppl will die without it :(\n Comment: Xenobots, but even better: [https://www.youtube.com/watch?v=aBYtBXaxsOw](https://www.youtube.com/watch?v=aBYtBXaxsOw)\n\nAnd with hallucinations in DNA code!\n Comment: Moth wings. On your elbow tips. Moth sized.\n Comment: ![gif](giphy|5YhFFUFq6ZTry|downsized)\n Comment: I think gene editing will also raise protests (even when we know it's safe). I have come to terms with the fact that there are people who will be against medicine, even if there is solid research behind it (as in the case of COVID vaccines). We won't do anything about it\n Comment: the people who want to edit their genes and the ones who were against covid vaccines are different people :)\n Comment: People largely became suspicious of those because of vaccine mandates. They'd be equally suspicious if politicians tried to push the idea that you shouldn't leave your house unless you edit your genes.\n Comment: Demis looks pretty good bald in general though, look at any of his pictures with hair and compare them to when he's bald. But Ilya... yeah lol\n Comment: 🙏\n Comment: I renounce hair on balls 🤦\n Comment: There is no one type of cancer. There is multiple types of cancers while baldness , predominantly male pattern baldness might be easier to cure.\n Comment: I’m bald and have leukemia, just advance the science please.\n Comment: \"The best I can do is to make people think baldness is cool\"\n Comment: maybe 🤔 when companies invest more in rl then they could do it ? Imagine agi like cultural transfer paper\n Comment: The mad scientists will just go to Colombia where there are no such restrictions.\n Comment: less then decades for sure, because regardless if this particular technology is used or not when ASI happens everything will be super accelerated.\n Comment: Wrong\n Comment: Best possible outcome.\n Comment: lol, sketch af\n Comment: I responded earlier, but I wanted to add, DNA is the human storage device & some would say it is a ‘radio’ communication device, sending and possible receiving capabilities. CRISPR aside, how about learning these capabilities of ATCG 😉 writing, I’ve read, DNA construction or duplication can be done via LFR, using base components of ATCG. Now that’s a mind fuck.\n Comment: sometimes I get unicorn horn under my armpit. Really unreliable model\n Comment: Lol\n Comment: That’s the real reason why he disappeared \n Comment: ilya has the bald rizz, you are just not b leeving hardenough\n\nhttps://preview.redd.it/800sptzaxvxc1.png?width=540&format=png&auto=webp&s=447d8717b9bf98aecdb712a28aed6c77750cb914\n Comment: >I’m bald and have leukemia,\n\nOh. I hope, you have a time. What doctors said? I hope, you'll not be forced to use cryonics just so close to AGI/ASI :(\n Comment: Why Colombia? Any source for saying that?\n Comment: Science takes patience!\n Comment: My thoughts exactly. I had a stem cell transplant and have stabilized in remission this year, so I am doing very well. Thank you for caring.\n:)\n Comment: [https://en.wikipedia.org/wiki/Medical\\_tourism](https://en.wikipedia.org/wiki/Medical_tourism)\n Comment: I live in Colombia 1/2 the year every year.\n\nYou do not need prescriptiosn for a host of medications like prozac, (not that I need it) Viagra, insulin, antibiotics and a host of othre drugs.\n\nAlso, I know for a fact that they approve many different kinds of experimental medicla procedures. Their equivalent of the FDA is quite laissez-faire and free market and pro-freedom. If a product cant' kill you, it's largely unregulated.\n Comment: not when you have an ASI and a bid deal of computing, it takes a lot less time.\n Comment: >I had a stem cell transplant and have stabilized in remission this year, so I am doing very well.\n\ngood to know. Hope we will have a good singularity, not a dark one, and everybody from us can have a proper treatment for their health issues.\n Comment: Even that can't predict any possible effect in the physiology of humans, caused by novel therapeutics. I think clinical trials will still be relevant.\n Comment: We don't even know if we will be able to align AI's values with ours...Relax. Breakthroughs will happen fast, but applications and commercial usage will take a little longer. If we go too fast, we might turn the whole humanity as a massive amoeba.\n Comment: I remain optimistic, but can’t help but rationalize with the history of absolute power. It’s easy to be evil, but it takes strength and vigilance to be good. All we can do is to work on self improvement, set examples of what you want to see in the world, and make effort. Some say love conquerors all, so we will see. Stress does not do us well so try not to fret.\n Comment: Yes, at least in the beginning, before quantum computing, and it maps everything down, clinical trials will still be the de facto process, but instead of having thousands? Of trials running, with something like an ASI we could probably run 10x-100x more trials at a time.\n Comment: Correction: Costa Rica will turn people into amoebas.\n Comment: You're right, the success rate will be higher, because AI does a better job to select and prescreen candidates."
    },
    {
        "title": "Nuggt: A LLM Agent that runs on Wizcoder-15B (4-bit Quantised). It's time to democratise LLM Agents",
        "link": "https://www.reddit.com/r/LocalLLaMA/comments/14qednx/nuggt_a_llm_agent_that_runs_on_wizcoder15b_4bit/",
        "text": "Well I dont know where to begin... Last month I started on this project called Nuggt because I was fed-up with how all the autonomous agents out there require GPT-4 (at least 3 months ago) and GPT-4 is expensive and I didnt have no API keys at that time. So I wanted to create something with GPT-3.5 and thats when this whole Nuggt story started.\n\nLong story short why stop there mate why not make it run on a open source model.. sounds crazy (for me at least cuz I am no AI legend). So every time a new LLM model came out I tested it with nuggt by adjusting my initial prompt. They all failed because models like Vicuna were good in imitating not reasoning (as highlighted by the ORCA paper).\n\nHowever, as some of you might have noticed, models trained coding for displayed some form of reasoning, at least that is what I noticed with StarCoder. Unfortunately, StarCoder was close but not good or consistent.\n\nToday, I have finally found our winner Wizcoder-15B (4-bit quantised). Here is a demo for you. In this demo, the agent trains RandomForest on Titanic dataset and saves the ROC Curve.\n\n&#x200B;\n\n[A LLM Agent training RandomForest on Titanic dataset](https://reddit.com/link/14qednx/video/peeme50s2y9b1/player)\n\nYou can find the github repo at: [https://github.com/Nuggt-dev/Nuggt](https://github.com/Nuggt-dev/Nuggt)\n\nDo check it out and give me your feedback.\n\nOKAY I CAN FINALLY SLEEP IN PEACE NOW GOOD NIGHT\n\n\\[EDIT\\]: Nuggt now supports Oobabooga API via cloudfare (credits to u/Ion_GPT)\n Comment: Nice thing, I am going to play with it. From a very quick look, I do not like that I have to run the model on the same machine, I usually run my models with Oobabooga on cloud using API mode.\n\nI will modify your code and add the option to use the model via booga API. I will open a PR when I am done, maybe will be others who would like this option.\n Comment: Looks great, nice demo/README and this is something I've been looking to do also (agents without \"open\"AI apis). Will check it out today.\n\nFor non-code models, what local models have you found to be the best at following these kind of agent-prompts? So far, I've found airoboros and Nous Hermes to be best.\n Comment: Now you can use this to automate itself to find even better models..\n Comment: Without diving into it more deeply, Nuggt looks fantastic!\n Comment: How much vram does this need?\n Comment: Can I ask what is the context window of WizardCoder? What is the maximal number of tokens that it can take as a prompt and generate?\n Comment: what do yall think can be some useful applications for Nuggt\n Comment: I'm excited to try this, I have a repo with a bunch of LLM generated webapps but I am using simple single-shot prompts (code @  https://github.com/the-crypt-keeper/llm-webapps demo @ https://huggingface.co/spaces/mike-ravkine/llm-webapps-results )\n\nBased on my tests so far, I expect 4-bit quantization to hurt performance quite a bit compared to even 5-bit.. I think 8-bit quants are the best option for coding models (a statement I have not yet published the data to back up so hold me to this)\n Comment: Can someone please make an open coder model trained on Solidity\n Comment: Cool stuff, not as familiar with the agents landscape as it's new but how does this stack up against other agents? is there any agent evals?\n Comment: What are the system requirements?\n Comment: Thank you for the project. After thoroughly examining the code, I attempted to apply it. However, I encountered challenges in generating the necessary processing steps irrespective of adjusting WizardCoder's parameters. In comparison, Vicuna33b 1.3 demonstrates significant improvement in this regard. Moreover, it appears that only Guanaco 65b possesses the ability to genuinely comprehend and generate steps based on the provided instructions.\n Comment: Thank you so much! That would be huge help. Looking forward to your PR\n Comment: Nuggt looks a lot like this other program AGiXT. AGiXT has that functionality.  Under the hood from what I can see on github they are two different ideas for sure but AGiXT already has the network ooba plug in. I can't wait for all these ideas to converge into something way to useful.\n Comment: >AGiXT\n\nReceived your PR, would review and merge today and you to the list of contributors. Really appreciate the work :)\n Comment: Merged! Thank you.\n Comment: Yes please.\n Comment: Don't underestimate vicuna, 1.3 has been very strong in my testing.\n Comment: >Nous Hermes\n\nWow you mentioned the two models I did not try HAHA I will check them out soon thank you for sharing. I have tried the following:  \n\n\n\\- Vicuna 7B/13B/33B (Version 1.1)  \n\\- MPT 7B/30B  \n\\- Guanaco 33B  \n\\- StarCoder/StarCoder Plus/StarChat Beta  \n\\- Falcon Instruct 7B/40B  \n\\- Orca Mini 7B/13B  \n\\- GPT4All  \n\n\nI will add airoboros and Nous Hermes to the list and share the results!\n Comment: LOL if only that worked..\n Comment: Thank you! :)\n Comment: I saw youtubers running it on RTX 3090 24G (To be exact: Gigabyte GeForce RTX 3090 VISION OC 24G NVIDIA 24 GB GDDR6X)   \n\n\nI usually just run it on cloud (anything between 20-48G works). However it runs on 4-bit quantised version so long story short if you can run Vicuna-13B 4-bit quantised then you are good to go!\n Comment: WizardCoder is trained on StarCoder which has context window of 8k!\n Comment: Creating HTML text and JSON data\n Comment: Have you looked into GPTengineer which uses chatGPT? Can this achieve similar functionality as that?\n Comment: First impressions from reading over the code (didn't try to use it yet):\n\n* Are you using google sheets forms as a ghetto logger?  I love this idea and I'm going to steal it and probably claim I came up with it :D\n\n* Love the quick and dirty socket API but its a low-value component vs the rest of this project. An autogptq wizardcoder \"openAI compatible\" REST API is available here: https://github.com/mzbac/AutoGPTQ-API/blob/main/blocking_api.py \n\nThe advantage to using this is that there's \"openAI compatible\" wrappers like this for many, many models and this would go a long way towards making nugget LLM-agnostic.\n\nWill give it a spin next and report back.\n Comment: >https://github.com/the-crypt-keeper/llm-webapps\n\nLLM Webapp Experiments looks quite exciting. I face similar issue as you that wizardcoder does not output the code in a simple block. That is what is stoping me from implementing the fix\\_error part of the project where the agent corrects its own code if there is an error. Let me know if you find a solution!  \n\n\n8-bit quants perform best HAHA! But i also realised that inference becomes slow so I chose 4-bit\n Comment: Train it on Lean4 instead\n Comment: Agent evals, i dont know of any maybe someone else can share if they know. I will to try search it up you have a good point here.   \n\n\nAs for comparing it to other agents, well I dont know about the rest.. but uhh my experience has not been the best HAHA I believe at this stage like whenever we prompt an agent we should have some sort of a debugging method. What i mean is something like, write a prompt -> see the result -> understand the result/cause of the result -> change the prompt accordingly to get a better result -> and so on..   \n\n\nI practiced this approach with nuggt. However, I did not do this with other agents maybe thats why my experience with other agents was not very good. I would spend more time in coming with an evaluation. Thanks for your comment\n Comment: The weights need around 8GB of vRAM. You need additional 2GB for generating text. So maybe 10-12GB of vRAM.\n Comment: Thank you for testing the project. You are right I encounter challenges for certain tasks as well. But I request you to try this out, it's a method that has worked for me. Instead of changing the model parameters, look at the logs (Steps/Reason/Action/Action Input/Observation) from the model and try to adjust your prompt to get better results. Hopefully you would get better results. Would post a tutorial on this soon. \n\n&#x200B;\n\nThank you for testing Vicuna and Guanaco, I would test it out as well and post the results here. But i also want to emphasize that even if they perform better than WizardCoder, my vision for the project is to make agents run successfully on smaller models because thats where I think the value is!\n Comment: Oh interesting I will take a look into AGiXT as well. Thanks for the info\n Comment: Oh yes v1.3 seems quite strong but I have only tried the 13B version for v1.3 What I noticed is that it has some problems sticking to the format (unlike v1.1). I dont know why, but will try once more specially with 33B and share the results\n Comment: Nous Hermes is the first one I can sometimes confuse for chatGTP in speed and output. It is the default for GPT4All now.\n Comment: >Thank you! :)\n\nYou're welcome!\n Comment: Oh noice love that, let me know if you need help with the prompts.. it takes a while to get them right\n Comment: I think you missed the point of the project.\n Comment: >ghetto logge\n\nHAHAH i used the google sheets to keep track of performance. Thanks for sharing the openAI compatible REST API. Would add it soon\n Comment: Did you see my wizardcoder prompt engineering? 😅. I ask it really nice to just give me code and it generally complies:\n\nhttps://github.com/the-crypt-keeper/llm-webapps/blob/main/projects.yaml\n\nWithout that prompt prefix it spits back step by step instructions instead of code quite often.\n Comment: 10-12GB of vRAM sounds about right! If you can run Vicuna 13B 4-bit quantised you can definitely run this!   \n\n\nI saw some youtubers use RTX 3090 24G\n Comment: I have made extensive attempts based on your proposal, fixing parameters in WizardCoder and adjusting prompts to achieve the desired format of output. However, due to the limited size of its instruction training dataset, WizardCoder exhibits a weaker ability to strictly adhere to the specified format. I tried incorporating a few-shot demonstration generated by GPT4, but despite its ability to loosely differentiate steps, it couldn't output in the desired format. Therefore, I propose employing a combined approach of few-shot learning and multi-turn dialogue, where the initial dialogue generates the steps along with their corresponding summaries, and subsequent steps are generated based on the summary. This approach holds potential to enhance the usability of WizardCoder; however, the generation of the \"Observation\" section remains a challenging aspect.\n Comment: I learned today that SuperAGI also has an ooba/koboldai plugin so another option to try.  I think we are hitting critical mass here with the context extensions. It's actually worth connecting in textgen to stuff now haha.\n\n&#x200B;\n\nOh and agixt is usually broke because it is moving so fast but one of the last updates I found to be stable. The version number was 1.2.25. JoshXT the guy who runs it is saying just install 1.2.25 till he's done with everything.\n Comment: They released a v1.3 13B and then a week later did a v1.3.0 that had fixes - did you try the fixed one?\n\nI test these models with both USER and HUMAN variant of prompts, it's not always clear which one performs better.\n Comment: I am going to test models tonight, would test Nous Hermes and update the results here\n Comment: Oh man sorry I missed this, I really want a good prompt that makes it output just the code for the self fix feature of nuggt. I will test your prompt tonight. If it works would ask you for a PR to give you the credit for the prompt!\n Comment: Just to check, are you using this model? TheBloke/WizardCoder-15B-1.0-GPTQ ?\n\n> model_name_or_path = \"TheBloke/WizardCoder-15B-1.0-GPTQ\"\n\nBecause the file itself is a bit larger than 8GB\n\ngptq_model-4bit-128g.safetensors\n9.2 GB\n\nMight be a bit tight for 12GB VRAM. I'll give it a try on my 3060 hopefully this week.\n Comment: This is interesting, thank you so much for testing and sharing the results. \n\nTo understand the issue better, I was curious if you tried the three examples provided in the repo? Also, it would help me better troubleshoot if you could share your prompt or the task that you were trying to complete with the agent.\n Comment: Oh i did not know about v1.3.0, thank you for mentioning this. I will test tonight and update the results here\n Comment: works pretty well with my 3060.\n Comment: Thanks for testing!\n Comment: Thanks for confirming!"
    },
    {
        "title": "No, LLM Agents can not Autonomously Exploit One-day Vulnerabilities.",
        "link": "https://struct.github.io/auto_agents_1_day.html",
        "text": "\n Comment: Good write up, author does a good job of making a clear link between the LLM, it's access to web lookup, and the careful selection of CVEs.  They draw the correct conclusion it's a powerful tool, but by no means novel or autonomous.\n Comment: Not with that attitude they can't.\n Comment: play on words...  0days, ok, 1days, you're DREAMING. of course it can.\n Comment: There are LLMs that are capable to exploit 1 day vulnerabilities. While GPT-4 is a general model that knows from everything something, there are also specialized models that were intentionally trained with malware data to write malware and exploits.\n Comment: Yes.. they can. And they are.\n Comment: Yea so… I know for a fact this is happening very soon and I’m not sure if it will be a good thing or a bad thing. Depends on certain factors.\n Comment: Yet...\n Comment: Yet\n Comment: Maybe not known publicly.\n Comment: Even allowing for the original authors' decision not to publish key details of the exploits, it's disappointing they provided so little information on their methods. It's sadly not uncommon - many CS papers lack much of the detail you'd need to reproduce the findings. (Good authors will link to extended versions of the paper that can be found elsewhere, but most don't.)\n Comment: Happy cake day!\n Comment: could you share more details or links on such specialized LLMs?\n Comment: Source of this actually working?\n Comment: They can barely handle standard coding tasks; if they were capable of more, programmers might soon find themselves redundant, as AIs would then start writing and refining their own code.\n\nThis notion is completely off the mark. It comes from those who wish to appear knowledgeable or ahead of the trends, assuming others are making such progress.\n\nThose who claim that they can do this, or are already doing it, invariably fail to provide any credible sources to back up their claims, leaving one to suspect it's all just fabricated.\n Comment: Not really. Threat actors are using LLMs to help with attacks (writing phishing emails, researching vulnerabilities, debugging code, etc) but I've never heard of an LLM being used as an A-Z automated attack tool.\n\nhttps://www.microsoft.com/en-us/security/blog/2024/02/14/staying-ahead-of-threat-actors-in-the-age-of-ai\n Comment: No.\n Comment: DARPA has servers that can find 0days and also defend and patch against 0day or attacks in real-time.\n\nThey brought em to DEFCON one year and had a whole contest with em.\n Comment: Stop sipping paint thinner.\n Comment: Trust you bro? How do you know for a fact?\n Comment: No it isn't.\n\nAt a high level there's four steps for an LLM to take for this to happen. \n\n1) understand the CVE    \n2) craft code to exploit it    \n3) locate vulnerable targets    \n4) execute the exploit correctly    \n\nLLMs can't reliably do any of these steps. It can maybe do 2) in very simple scenarios, if you hand-feed it the data necessary. We are nowhere near this being a fully autonamous process. If we were, security firms would be beating down my door to sell it to me as a defensive tool. We'd be replacing red teams left and right.\n\nTo put it plainly, if this was even close to possible, *capitalism would already be trying to sell it to you*.\n Comment: Ignore this one - not happening soon at all.\n\nLLM as quite dumb for now\n Comment: The source will never come  \n Comment: This seems like a more accurate depiction and lines up with other ai tools used in the tech industry. Doing a portion of the work load but still requiring human intervention for bigger decisions.\n Comment: At least not the ones publicly available \n Comment: Ok Russia\n Comment: That’s awesome I didn’t know about this\n Comment: 😂🤣😂\n Comment: AI farms are desperately trying to get profitable. They're having trouble finding real world problems they can reliably solve autonomously. There is no \"secret super AI\", they would throw it in our faces immediately.\n Comment: Yeah exactly. AI systems can easily be built now and with gpt-4-like internal builds running autonomously I’m sure it’s possible and happening.\n Comment: It's most likely a technology like quantum computers, where it's a really cool idea, but fails at any practical use untill they find a widespread use for it. Its all fun and games to know electricity exists but knowing how to make a light bulb with it may take awhile\n Comment: Just because you don’t understand what quantum computing can be used for doesn’t mean others haven’t found important uses and are actively doing so\n Comment: It's more about widespread application of a revolutionary technology, I'm aware of what they are used for.\n Comment: What are they used for?"
    },
    {
        "title": "[R] Data Interpreter: An LLM Agent for Data Science",
        "link": "https://www.reddit.com/r/MachineLearning/comments/1bdxqt4/r_data_interpreter_an_llm_agent_for_data_science/",
        "text": "Abstract:\n\n>Large Language Model (LLM)-based agents have demonstrated remarkable effectiveness. However, their performance can be compromised in data science scenarios that require real-time data adjustment, expertise in optimization due to complex dependencies among various tasks, and the ability to identify logical errors for precise reasoning. In this study, we introduce the Data Interpreter, a solution designed to solve with code that emphasizes three pivotal techniques to augment problem-solving in data science: 1) dynamic planning with hierarchical graph structures for real-time data adaptability; 2) tool integration dynamically to enhance code proficiency during execution, enriching the requisite expertise; 3) logical inconsistency identification in feedback, and efficiency enhancement through experience recording. We evaluate the Data Interpreter on various data science and real-world tasks. Compared to open-source baselines, it demonstrated superior performance, exhibiting significant improvements in machine learning tasks, increasing from 0.86 to 0.95. Additionally, it showed a 26% increase in the MATH dataset and a remarkable 112% improvement in open-ended tasks. The solution will be released at [https://github.com/geekan/MetaGPT](https://github.com/geekan/MetaGPT).\n\n&#x200B;\n\nhttps://preview.redd.it/6bcww0qb15oc1.png?width=1280&format=png&auto=webp&s=d98f2e05fbdf06f186b93782a786dc94b3d33bac\n\n&#x200B;\n\nhttps://preview.redd.it/565u97cc15oc1.png?width=1116&format=png&auto=webp&s=fdefeda7c7733e610e0984ba7d7b77024d91a1b0\n\n&#x200B;\n\nhttps://preview.redd.it/a4c6lopc15oc1.png?width=1150&format=png&auto=webp&s=8b1e7cc27f3a2a9b75a66da0fdd54d29bf988f86\n\n&#x200B;\n\nhttps://preview.redd.it/lab3uh2d15oc1.png?width=731&format=png&auto=webp&s=9f1506e607eb644b77bd2ba22e2189d005e1c010\n Comment: There are some really concerning things in the task flow above just on inspection:\n\n'Correlation Analysis' and 'Exploration'.  Is our dataset 'well behaved' and has enough samples to avoid a multiple comparison problem or a poor modeling decision based on noise in the data?  Automating this is exceptionally dangerous-and poor modeling practices are rife across industry amongst ml practitioners.  Should we expect gpt models to avoid these? How are the use of priors leveraged here?\n\nImputation.  This is also a part where prior knowledge is necessary to build good imputations.  Simple imputation can go horribly wrong or give results that are misleading.  In practice, you have non missing at random or complex missingness structures that simple imputation will miss the mark on.  How does this agent arrive at its choice of imputation framework given a problem?\n\nFeature Selection:  Feature selection is notoriously unreliable-'data driven' methods often produce spurious findings , and people often conflate good predictive features using in sample measures as good casual predictors-this is far from the truth.  How does this task flow address questions like these, and do so in a manner that avoids the end user-here perhaps a non practitioner with a good stats background not being over confidant?\n\nWithin the project scope choosing loss functions that is appropriate for your problem cost function; determining a candidate model's utility over another one.  Tradeoffs in prediction vs calibration etc etc\n Comment: Why does this have over 200 up votes with 1 comment on a post where the abstract completely avoids saying what is done by using vague buzzwords?\n Comment: Can anyone see my comment? It's strange why my comments are not displayed\n Comment: Data Interpreter has achieved state-of-the-art scores in machine learning, mathematical reasoning, and open-ended tasks, and can analyze stocks, imitate websites, and train models.  \n  \nData Interpreter is an autonomous agent that uses notebook, browser, shell, stable diffusion, and any custom tool to complete tasks.  \n  \nIt can debug code by itself, fix failures by itself, and solve a large number of real-life problems by itself.  \n  \nWe open-source our code and provide a wealth of working examples to give everyone access to state-of-the-art AI capabilities.\n Comment: Nice work from the team!\n Comment: Sounds good, does this seem to allow software companies to take their capabilities even further? Or to put it another way, can I use it in conjunction with a software company?\n Comment: Thank you for your interest, and you've made an excellent point. Both data analysis and machine learning modeling processes rely heavily on reliable data feedback and domain-specific prior knowledge for guidance. In real-world scenarios, the human-machine learning modeling process undergoes several rounds of iterative debugging to refine the choice of operators and hyperparameter settings throughout the model development. Our initial efforts have explored integrating Large Language Models (LLMs) into the workflows of data analysis and machine learning modeling, enhancing LLM's ability to manage task dependencies and updates, as well as improving the integration of tools for navigating complex workflows and data challenges. We've also encountered challenges in optimizing outcomes and are currently working on iteratively improving results based on solid numerical feedback, aiming for automatic enhancements to the modeling process. We invite you to keep up with our ongoing work.\n Comment: this is gonna sound mean:but maybe bots or laypeople who think just producing a model is 'datuh sceinse'.  it's super odd that there's no discussion.  Something seems shady.  however-this sub also has a large non technical or practitioner audience now\n\nautomating a workflow-in the flavor of kaggle(which no doubt serves as a training corpus) is dangerous.  being 'good at kaggle' is a big ol red flag a lot of the time-and the screenshots make this look like a typical kaggle approach.\n\nedit: check out the post histories of the other posters\n Comment: there are SO many cases of posts like this on here. must be something fishy going on\n Comment: I don’t seem to see your other comments\n Comment: fine. It seems to be an old problem with reddit.\n\nAnyway, I'm wondering how I know \"better\" now that I can't use Devin. I saw this description on your Twitter."
    },
    {
        "title": "Launching AgentSearch - A local search engine for your LLM agent",
        "link": "https://www.reddit.com/r/LocalLLaMA/comments/18ntozg/launching_agentsearch_a_local_search_engine_for/",
        "text": "Hey everyone,\n\nI've been part of this community for a while and have gained a lot from your insights and discussions. Today, I'm excited to share a project I've been working on called AgentSearch. The idea behind this is to make the vast scope of human knowledge more accessible to LLM agents.\n\n&#x200B;\n\nWe've started by embedding text from sources like Wikipedia, Arxiv, and filtered common crawl. The result is a massive database of over 1 billion embedding vectors. The dataset will be released to the public, but right now I am working out logistics around hosting the 4 TB+ database.\n\nYou can check out the search engine at \\[[search.sciphi.ai](https://search.sciphi.ai)\\]([https://search.sciphi.ai](https://search.sciphi.ai)). I'm also sharing the source code for the search engine at \\[[github.com/SciPhi-AI/agent-search](https://github.com/SciPhi-AI/agent-search)\\]([https://github.com/SciPhi-AI/agent-search](https://github.com/SciPhi-AI/agent-search)), so anyone who wants to can replicate this locally.\n\n&#x200B;\n\nAnother part of this project is the release of a model called Sensei, which is tailored for search tasks. It's trained to provide accurate and reliable responses and to return the result in JSON format. You can find Sensei at \\[HuggingFace\\]([https://huggingface.co/SciPhi/Sensei-7B-V1](https://huggingface.co/SciPhi/Sensei-7B-V1)).\n\n&#x200B;\n\nThis project represents a big step in the dataset of embeddings, thanks to some new initiatives like RedPajamas. With Sensei, we're aiming to offer a tool that can handle search-based queries effectively, making it a useful resource for researchers and general users. Sensei is available for download, and you can also access it via a hosted API. There's more detailed information in the \\[documentation\\]([https://agent-search.readthedocs.io/en/latest/api/main.html](https://agent-search.readthedocs.io/en/latest/api/main.html)).\n\n&#x200B;\n\nAgentSearch and Sensei will be valuable for the open source community, especially in scenarios where you need to perform a large number of search queries. The dataset is big and we plan to keep expanding it, adding more key sources relevant to LLM agents. If you have any suggestions for what sources to include, feel free to reach out.\n\n&#x200B;\n\nI'm looking forward to hearing what you think about this project and seeing how it might be useful in your own work or research! \n\nThanks again.\n Comment: This is intriguing, thank you for sharing with us! I have a couple of questions\n\n* Is this project limited to data that already exists in an database accessible to the model, or is it capable of performing internet searches? For example would I be able to ask it to get the latest weather in xyz city?\n* Is this project limited to OpenAI chatgpt as the model provider or would we be able to point it at a local llm server such as ollama, llama.cpp, lmstudio, etc. Would we need an OpenAI proxy like litellm if we want to use a local inference/backend for the llm server?\n* Is this project capable of retaining and remembering new information for recall later, similar to projects like memgpt and autogen aim to do?\n Comment: Very interesting! Looking forward to the full release. What embedding model are you using? What chunk sizes?\n Comment: What a great initiative! thanks for the effort.\n Comment: What an awesome project, congrats! What tools did you use to create such a massive embedding vector database? And how many computing resources did it take?\n Comment: Congratulations, this is really impressive!\n Comment: Sensei looks great! Would it be possible to distil it even further down? 7B is still reasonably chunky\n Comment: \"This model strives to specialize in using search, such as AgentSearch, to generate accurate and well-cited summaries from a range of search results, providing more accurate answers to user queries.\" \n\nWhat is the overall retrieval accuracy based on the question / expected response? Have you benchmarked this at all? If so what was your validation method? And how much of an improvement was this over traditional methods?\n Comment: What's the link for your API? I can't find it documented anywhere. I think I have it correct but I keep getting detail: Not Found responses.\n Comment: Wow, that's impressive to say the least! I've really been feeling that limited depth of knowledge in a lot of subjects is a significant detriment to a local model's overall utility. This seems like it'd be a major step forward in that regard.\n Comment: Amazing work. I'd love to test this up against other open source models. Would you consider uploading the v1 dataset as a .torrent file? Or is there another recommended way if I wanted to set this up locally... \n\n  \nTo me, this is one of the most fascinating pieces and I'd like to run more direct comparisons with Sensei, Mistral, Phi, etc.\n\n&#x200B;\n\nThanks again!\n Comment: Ah. I was hoping that you had made an app that let's a local model use the files on my system, to discuss their texts and locate them.\n Comment: >Is this project limited to data that already exists in an database accessible to the model, or is it capable of performing internet searches? For example would I be able to ask it to get the latest weather in xyz city?\n\nYou can easily connect the model to Google, for example. There is ready made support for this in the agent-search package if you use SERP API. I could make a tutorial if there is interest.\n\nMoreover, on the web app you can toggle the button which reads \"SciPhi RAG\" to use Google as a RAG provider.\n\nThis project isn't dependent upon OpenAI proprietary models in any capacity. IT does use the OpenAI Python package as a dependency, but only for compatibility with vLLM which allows me to serve the model with an OpenAI ready format.\n\nAs for bullet point 3, not really at this time.\n Comment: >Very interesting! Looking forward to the full release. What embedding model are you using? What chunk sizes?\n\ngreat questions - Jina Base, 512. I wanted to do multiple chunk sizes but quickly realized the dataset was already growing too large.\n Comment: thank you!\n Comment: >What an awesome project, congrats! What tools did you use to create such a massive embedding vector database? And how many computing resources did it take?\n\nThanks!\n\nI coded everything up from scratch, more or less, except for database platforms. I used qdrant and postgres to manage to the vector and relational databases. There is surely a way to do both of these at the same time, but I was struggling to get this to perform as well.\n Comment: >\"This model strives to specialize in using search, such as AgentSearch, to generate accurate and well-cited summaries from a range of search results, providing more accurate answers to user queries.\"What is the overall retrieval accuracy based on the question / expected response? Have you benchmarked this at all? If so what was your validation method? And how much of an improvement was this over traditional methods?\n\nty\n Comment: yes, this is on the short term road map - I would like to pack all the power into a phi-sized model.\n Comment: I'm working on putting together a more comprehensive academic paper as well as some more advanced work using AgentSearch, I will put an update when that is ready.\n\nIt is not as straightforward to answer questions like this without some thought at this moment since most benchmarks are not designed very well for RAG.\n Comment: apologies, the relevant bit in the docs got nuked - it is [api.sciphi.ai/search](https://api.sciphi.ai/search) and [api.sciphi.ai/v1/](https://api.sciphi.ai/v1/)... for completions.  I will update the docs [here](https://agent-search.readthedocs.io/en/latest/api/main.html) today to make this very clear.\n Comment: I am a bit of a newb here. How long until we have AI's own conclusions add to depth of knowledge, will it be done indirectly by waiting for AIs to help research papers or recursively in datasets or something else?\n Comment: Torrent would be good, if someone wants to download from HF and upload I would be greatful and help seed.\n\n  \nOtherwise, I could try to get to that myself in the near future.\n\n&#x200B;\n\nI have put together an API, because I understand how difficult it is to stand up the search engine (as I had my own pains in doing so). You can see the docs here - [https://agent-search.readthedocs.io/en/latest/](https://agent-search.readthedocs.io/en/latest/).\n\n&#x200B;\n\nIf you get a chance to try it out please let me know your findings :)\n Comment: >Ah. I was hoping that you had made an app that let's a local model use the files on my system, to discuss their texts and locate them.\n\nah, not yet - that could be an interesting direction to go in, I'm thinking about adding scripts that would let people include their own bespoke knowledge into the engine.\n Comment: Thank you for the clarifications! Pretty amazing stuff. I'll be following the progress with this project going forward :)\n Comment: I just noticed Sensei is the name of an Adobe image AI\nIt might make sense not to call it that too\n Comment: thanks!\n Comment: Update: I've made 2 attempsts, but have been unable to get a working .git repository.\n\nWith the multiple tried I have about 1.2TB of data, but no way to really piece apart what did and didn't make it.  \n\n\nMy brother may have access to a stronger connection that should be more stable for a download of this size.  I'll follow up with any questions, but I haven't given up on the idea quite yet.\n Comment: You've got a lot on your plate already. Good luck.\n Comment: >Update: I've made 2 attempsts, but have been unable to get a working .git repository.With the multiple tried I have about 1.2TB of data, but no way to really piece apart what did and didn't make it.My brother may have access to a stronger connection that should be more stable for a download of this size.  I'll follow up with any questions, but I haven't given up on the idea quite yet.\n\nHey Mason,\n\n&#x200B;\n\nI would love to help debug your attempt here - it might make sense to do this procedure in steps. It's good that you have downloaded so much of the data, but now you should start turning the crank to see if you can put it into a searchable database.\n Comment: Good advice - and I agree!\n\nI can start digging into this - especially if you're up for helping debug! I've got some experience setting up local LLMs and RAG implementations, but in full transparency have not built a dataset from scratch so I'll need some guidance!  \n\n\nAdditionally, I've found a more-stable internet connection and am running another attempt at a full download. If it's successful, I'm still interested in seeding a torrent that we're confident is 1:1 with the agentsearchv1 dataset. So stay tuned there...\n Comment: I did finally get this downloaded, packaged as a torrent file, and wrote about it here if anyone wants to seed: [https://masonjames.com/publishing-the-agentsearch-v1-torrent/](https://masonjames.com/publishing-the-agentsearch-v1-torrent/)\n Comment: update: download from hf complete!\n\n2.5tb downloaded in total. now... to create that torrent and build a local search-engine (with some help, hopefully!)"
    },
    {
        "title": "[P] Nuggt: A LLM Agent that runs on Wizcoder-15B (4-bit Quantised). It's time to democratise LLM Agents",
        "link": "https://www.reddit.com/r/MachineLearning/comments/14qo8a2/p_nuggt_a_llm_agent_that_runs_on_wizcoder15b_4bit/",
        "text": "Hi everyone,\n\nI wanted to share my open source project Nuggt.\n\nIn the past few months, we have seen a lot projects regarding Autonomous Agents that run on Large Language Models. Some examples are BabyAGI, Auto-GPT, etc\n\nHowever, most of these models use GPT-4 which is very expensive and not everyone has access to GPT-4.\n\nSo we decided to play around with some open source LLM models that could run locally. We wanted to explore if we could create agents with these open source models and have them perform well...\n\nLong story short after trying out many models like Vicuna-13B, MPT-13B, StarCoder... most of them failed.\n\nToday, I have finally found our winner Wizcoder-15B (4-bit quantised). Here is a demo for you. In this demo, the agent trains RandomForest on Titanic dataset and saves the ROC Curve.\n\n[A LLM Agent training RandomForest on Titanic dataset](https://i.redd.it/ayafi6j5u2ab1.gif)\n\n[Check out the Github Repository](https://github.com/Nuggt-dev/Nuggt)\n\n[Join the Discord](https://discord.com/invite/YZp6jmFr)\n\n\\[EDIT\\]: The previous post was not clear as rightly pointed out by many so I have made the post shorter.\n Comment: I think you should have slept before you made this post I’ve no idea wtf you’re sharing with us\n Comment: Your vid link doesnt work, can you just explain what your thing does?\n Comment: I'm a bot, *bleep*, *bloop*. Someone has linked to this thread from another place on reddit:\n\n- [/r/datascienceproject] [Nuggt: A LLM Agent that runs on Wizcoder-15B (4-bit Quantised). It's time to democratise LLM Agents (r\\/MachineLearning)](https://www.reddit.com/r/datascienceproject/comments/14qw6ja/nuggt_a_llm_agent_that_runs_on_wizcoder15b_4bit/)\n\n&nbsp;*^(If you follow any of the above links, please respect the rules of reddit and don't vote in the other threads.) ^\\([Info](/r/TotesMessenger) ^/ ^[Contact](/message/compose?to=/r/TotesMessenger))*\n Comment: Can I use this commercially? WizardLM is marked as non-commercial last time I checked.\n Comment: What do you really mean when you say democratize the llms ?\n Comment: how much VRAM would i need to run this? the local wizcoder version\n Comment: Neat! Love a local LLM. Is your plan to let folks make use of this network as a commercial product, ie integrate it into/ distribute it with their products?\n Comment: Getting a lot of flak on this on OP, but I don’t know why. Great work. Is the agent built on top of langchain? Was that a real-time demo? What are your hardware reqs to run this locally?\n Comment: What do you think are some use cases for Nuggt? Happy to hear :)\n Comment: I am sure your shadow lovers will be smiling over you and grateful for your work while you are sleeping. Maybe I should get some AI to help explain your AI to me :) later perhaps. I also need more sleep, but first, food and a break.\n Comment: Is this an LLM talking???\n Comment: [removed]\n Comment: Really cool project!\n Comment: [removed]\n Comment: Can you list all the models you tried and why the others didn’t work?\n Comment: What failed for you with the other models (Vicuna, MPT, Starcoder)? Was performance not up to expectations?\n Comment: I watched the video and just got more confused\n Comment: You're right! I've made the post shorter (and less dramatic), hope it makes sense. \n\nHappy to answer any specifics!\n Comment: Absolutely, we're developing cost-effective AI agents using local models, designed to perform tasks on par with GPT-3.5. \n\nOur ultimate aim is a locally-operated AI personal assistant, capable of automating virtually any task you might have. \n\nThis demo displays just one of many possible automations achievable with local agents.\n Comment: Take a look at this, the length of the first 3 words in u/TotesMessenger comment are consistent with the first 3 digits of pi. This was only the case for 2745 comments out of 877689.\n Comment: Currently, this model is not available for commercial use. However, our team is actively looking out for new models with commercial licenses to incorporate into Nuggt.\n Comment: I'm not OP, and OP wrote \"democratize agents\" rather than LLMs. But it should be the obvious: creating tools and resources to give anyone access to run a local AI agent.\n Comment: I saw youtubers running it on RTX 3090 24G (To be exact: Gigabyte GeForce RTX 3090 VISION OC 24G NVIDIA 24 GB GDDR6X) I usually just run it on cloud (anything between 20-48G works). However it runs on 4-bit quantized version so if you can run Vicuna-13B 4-bit quantized then you are good to go!  So maybe 10-12GB of vRAM.\n Comment: Yes, we're proactively preparing for commercial models, developing a platform where users can build workflows for their specific personal or business needs. Though WizCoder isn't for commercial use, our upcoming product is geared towards this. More details will follow soon.\n Comment: Hi u/nutin2chere, thanks for your kind words!\n\n\\- The agent isn't entirely built on Langchain; we went for an approach that offers more flexibility for our current implementation and anticipated future needs.\n\n\\- Yes, the demo was indeed real-time, but the video was sped up!\n\n\\- The weights require approximately 8GB of VRAM, plus an additional 2GB for text generation, totaling around 10-12GB of VRAM.\n Comment: Your post seems like it can train ml models.\n\nSeems to resemble code interpreter from chatgpt.\n\nWondering if you ran any test for generic data analysis (explain the trend if X in the data, what's the average of Y, etc)?\n Comment: You should try out Nuggt to understand Nuggt haha :)\n\nJokes aside, I've updated the post, hope it explains things better!\n Comment: thanks, appreciate it!\n Comment: thank you :)\n Comment: Why is there an exception crash in the video? How does that demonstrate a useful agent?\n Comment: What are your plans with respect to the risk of prompt injection in personal agents?\n Comment: Got it, thanks! so it's an agent framework with productivity tool use as the primary design goal\n Comment: Awesome. Again, great work and thanks for sharing. Keep pushing!\n Comment: u/Smallpaul, thanks for point this out! The exception was caused by an asyncio error on Jupyter notebook - nothing related to the agent. Will add handling for this.\n Comment: Hey u/Smallpaul, sflr - we plan to build Nuggt for users to quickly test out different defense/offensive mechanisms to combat prompt injections. \n\nSome of the techniques that we'll bring in are found here: https://learnprompting.org/docs/category/-defensive-measures"
    },
    {
        "title": "LLM agents can autonomously exploit one-day vulnerabilities",
        "link": "https://www.reddit.com/r/ArtificialInteligence/comments/1ce77bi/llm_agents_can_autonomously_exploit_oneday/",
        "text": "Researchers presented AI agents with security vulnerabilities and prompted them to exploit these\r. A GPT-4 agent was successful in autonomously exploiting 87% of the tested vulnerabilities\r. These capabilities could enable lower-skilled bad actors to engage in cybercrime.\r  \n\r  \nSource: https://dailyai.com/2024/04/llm-agents-can-autonomously-exploit-one-day-vulnerabilities/\r  \n\r  \n\n Comment: ## Welcome to the r/ArtificialIntelligence gateway\n### News Posting Guidelines\n\n---\n\nPlease use the following guidelines in current and future posts:\n\n* Post must be greater than 100 characters - the more detail, the better.\n* Use a direct link to the news article, blog, etc\n* Provide details regarding your connection with the blog / news source\n* Include a description about what the news/article is about. It will drive more people to your blog\n* Note that AI generated news text is all over the place. If you want to stand out, you need to engage the audience\n\n###### Thanks - please let mods know if you have any questions / comments / etc\n\n*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/ArtificialInteligence) if you have any questions or concerns.*\n Comment: then llm agents can autonomously patch one-day vulnerabilities\n Comment: Absolutely, here's a breakdown of how LLM agents can autonomously exploit one-day vulnerabilities:\n\nOne-Day Vulnerabilities:\n\nImagine a weak spot in a computer system's armor. This weakness is a vulnerability, and if someone discovers it, they can exploit it to gain unauthorized access or control. One-day vulnerabilities are particularly dangerous because they've been discovered but not yet patched. This window of time between disclosure and fixing the issue creates a risk window that attackers can try to exploit.\n\nLLM Agents:\n\nLLM stands for Large Language Model. These are powerful AI models trained on massive amounts of text data. They can generate text, translate languages, write different kinds of creative text, and even answer your questions in an informative way, just like I'm doing now!\n\nThe Research:\n\nResearchers at the University of Illinois Urbana-Champaign conducted a study to see if LLMs could be used for malicious purposes. They created an LLM agent specifically designed to target one-day vulnerabilities. This agent was given access to information about the vulnerabilities, including descriptions from databases like the Common Vulnerabilities and Exposures (CVE). The agent was also equipped with tools to interact with computer systems, such as simulating web browsing and file manipulation.\n\nThe Results:\n\nThe results were startling. The LLM agent they created was able to autonomously exploit a staggering 87% of the one-day vulnerabilities it was tested on. This means it could potentially figure out how to take advantage of these weaknesses without any human intervention.\n\nImportant Considerations:\n\nThis research was conducted in a safe, controlled environment. There's currently no evidence of LLM agents being used for malicious purposes in the real world.\n\nThe study focused on one-day vulnerabilities, which are not the only kind. Zero-day vulnerabilities, for instance, are even more dangerous because they are completely unknown. There's no indication yet that LLMs can exploit these on their own.\n\nThis research highlights the potential dangers of powerful AI and the need for careful development and deployment. Security researchers are now more aware of this potential threat, which can help them develop better defenses.\n\nOverall, this research is a wake-up call for the cybersecurity community. It highlights the need for robust security measures and staying up-to-date on the latest vulnerabilities. It also emphasizes the importance of responsible development and deployment of AI to minimize the risk of malicious use.\n Comment: that's it, security vs. privacy is an on-going, never-ending battle of tug-of-war.\n Comment: It's just now going to be a war that is fought by AI agents on behalf of people, with the fastest agent winning.\n Comment: While the race has always been a race, because organizations are often reluctant to spend money on cybersecurity/IT, the advantage afforded to malicious actors becomes more significant.\n Comment: It will be interesting to see how the reduction in cost due to renting cyber security AI agents from a company vs hiring IT. \n  \nI imagine most corporations will retain IT leads and they will likely keep a compliment of staff for bureaucratic purposes, but all these tech spaces are going to shrink.\n Comment: >but all these tech spaces are going to shrink.\n\nPossibly. But from the perception of a malicious actor, it's probably easier to outsmart someone who is monitoring those AI agents than senior analysts. What ends up happening is that the campaigns become more sophisticated as they attempt to leverage user error. And what will also end up happening is that AI cybersecurity companies will become bigger targets. \n\nI think AI will help to increase parity in some areas, but because most computer problems are due to user error/human involvement, and humans are still involved in those processes, those spaces won't change too much.\n Comment: Good take"
    },
    {
        "title": "[R] CRISPR-GPT: An LLM Agent for Automated Design of Gene-Editing Experiments",
        "link": "https://www.reddit.com/r/MachineLearning/comments/1cgyccx/r_crisprgpt_an_llm_agent_for_automated_design_of/",
        "text": "A new paper introduces CRISPR-GPT, an AI-powered tool that streamlines the design of CRISPR-based gene editing experiments. This system leverages LLMs and a comprehensive knowledge base to guide users through the complex process of designing CRISPR experiments.\n\nCRISPR-GPT integrates an LLM with domain-specific knowledge and external tools to provide end-to-end support for CRISPR experiment design.\n\nThe system breaks down the design process into modular subtasks, including CRISPR system selection, guide RNA design, delivery method recommendation, protocol generation, and validation strategy.\n\nCRISPR-GPT engages users in a multi-turn dialogue, gathering necessary information and generating context-aware recommendations at each step.\n\nTechnical highlights:\n\n1. The core of CRISPR-GPT is a transformer-based LLM pretrained on a large corpus of scientific literature related to gene editing.\n2. Task-specific modules are implemented as fine-tuned language models trained on curated datasets and structured databases.\n3. The system interfaces with external tools (e.g., sgRNA design algorithms, off-target predictors) through APIs to enhance its capabilities.\n4. A conversational engine guides users through the design process, maintaining coherence and context across subtasks.\n\nResults:\n\n1. In a trial, CRISPR-GPT's experimental designs were rated superior (see the human evals section of the paper for more).\n2. The authors successfully used CRISPR-GPT to design a gene knockout experiment targeting four cancer genes in a human cell line and it **successfully knocked them out**, demonstrating its practical utility.\n\nThe paper ([arxiv](https://arxiv.org/pdf/2404.18021)) also discusses the implications of AI-assisted CRISPR design, including its potential to democratize gene editing research and accelerate scientific discovery. However, the authors acknowledge the need for ongoing evaluation and governance to address issues such as biases, interpretability, and ethical concerns.\n\n**TLDR:** LLMs can guide humans on how to use CRISPR gene editing to knock out cancer cells.\n\n[More info here](https://open.substack.com/pub/aimodels/p/they-taught-ai-to-edit-genes-with) .\n Comment: Can’t wait for it to hallucinate me some additional genes. Garage CRISPR experiments are about to get SPICY.\n Comment: Putting an LLM on this process is so unnecessary. There's only 4 actual genome editing types that this tool designs guides for; you can decide the best one to use using a simple if-then decision tree, then call the same APIs this tool queries.\n\nEdit to add: the one example multiplex edit highlighted failed to look for large chromosomal rearrangements, which would be difficult to detect with the NGS assay they used.\n Comment: Where GitHub link?\n Comment: Honestly this is kind of disappointing, it's just an LLM finetune attached to some domain-specific tools.\n\nAren't there literal exabytes of DNA sequences (mostly plants/bacteria) sitting in gene databases? I'd be very interested to see what you can do with a model trained on that. Could we AI-generate some drought-resistant wheat by conditioning the generation \"in the style of\" a cactus?\n Comment: Such a tool is already publicly available at www.biomodai.com\nBut I think they're still actively expanding the platform. Exciting field!\n Comment: No value in this.\n Comment: I'm concerned they didn't highlight that this tool, if it actually works as promised, could be used in bioengineering a pandemic pathogen.\n Comment: AI can't ~~draw~~ bioengineer hands?\n Comment: Yeah if a general purpose programmer uses it, but seems like N exceptional efficiency gain for someone doing bioengineering everyday.\n Comment: not in paper :(\n Comment: [deleted]\n Comment: look at Evo\n Comment: I think Project CETI is more likely to succeed first, giving us Whale / animal LLMs before Gene LLMs.  And I also don't think CETI will succeed anytime soon, as much as I'd like it to.\n\n:/\n Comment: ok, thanks for letting us know!\n Comment: the amount of downvotes show how little this sub knows about biology. this is just an unnecessary gpt wrapper again.\n\nif you want to look at some use of nucleotide sequences as the language of biology in LLMs (which this paper isnt), look at Evo\n Comment: Knowing how well AI does with fingers.. I'm imagining everyone will have three arms and 8 fingers on each hand moving forward.\n Comment: Agreed. I'm more talking about all the news stories and netflix series about non-professionals experimenting with CRISPR and testing on themselves.\n Comment: No lizard tail for me then 🥺\n Comment: A lot of today's GMO crops were made by just [copy-pasting individual genes](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6651533/) from other plants or bacteria. \n\nA generative model should at least be able to do that, and I would expect it to be able to do much more abstract things that involve making small changes to many genes.\n Comment: Why no GitHub ???\n Comment: But AI has become a lot better at fingers now. The \"lol it can't do hands\" thing is getting rather old.\n\nSame with the \"but hallucinations!\" Objection. Yes, LLMs hallucinate. We're getting a handle on how to minimize or work around that. For example one of the points listed in the summary was:\n\n> The system interfaces with external tools (e.g., sgRNA design algorithms, off-target predictors) through APIs to enhance its capabilities.\n\nSo much like the search engine LLMs, there's a source of external \"truth\" that can keep them grounded.\n Comment: …yet\n Comment: [deleted]\n Comment: Fingers aren't quite the same as hallucinating something as innocuous as a base-pair change, when we haven't even scratched the surface of gene to protein to function mapping ...\n Comment: If they can't reliably explain what and why they do (self-reflection), it's likely that it won't be allowed anywhere near human medicine simply for liability reasons. It's the same with all the diagnostic startups, AI-powered X ray machines etc.\n\nIt might be useful as an experimental tool to quickly design and perform experiments and identify genetic features that can later be manually confirmed. In fact, given the vastness of most genomes, some kind of automated/AI mapping might be the only effective tool to obtain a functional map.\n Comment: It was just a joke my friend. Very on-topic if I might add.\n Comment: Never gonna happen. You're gonna need a whole symbiotic smart system to grow that tail out. Gotta make the meat smart.\n Comment: And I bet research scientists for 3D rendering would have told me it's impossible to generate a chair [\"in the style of spaghetti\"](https://imgur.com/fMQD1Up) just by doing statistics on a bunch of images of chairs and spaghetti. But here we are.\n Comment: But what I'm saying is that these systems won't likely go straight from LLM to finished gene sequence in the first place. The LLM isn't going to say \"make this sequence: AGCGGA...\" and the printers immediately rattle off whatever it asked for. This is about using LLMs as part of a larger system for designing experiments. Those other parts can be responsible for the specific sequences.\n Comment: But do you necessarily need to go through the path of building up genes -> proteins -> machinery -> cells -> morphology? \n\nIn NLP for example, people thought you'd have to build parse trees and solve sentence structure and build up to an understanding of language out of lower-level abstractions. Same for computer vision with edge detectors, SIFT features, etc. \n\nThat never worked (the real world is too messy for our brittle abstractions), and today it's all abandoned in favor of large-scale statistics on raw data. Maybe genomics will go the same way.\n Comment: > It might be useful as an experimental tool to quickly design and perform experiments and identify genetic features that can later be manually confirmed.\n\nThat's exactly what the paper this post is about is proposing.\n Comment: [deleted]\n Comment: Thanks, you too 👍"
    },
    {
        "title": "Nuggt: A LLM Agent the runs on WizardCoder-15B (4-bit quantised). It's time to democratise LLM Agents",
        "link": "https://www.reddit.com/r/singularity/comments/14r0x44/nuggt_a_llm_agent_the_runs_on_wizardcoder15b_4bit/",
        "text": "Hello Everyone, I want to share my open source project/experiment called nuggt with you.\n\n[An Agent training RandomForest on Titanic Dataset](https://i.redd.it/6a9cc5u6y2ab1.gif)\n\nIn the past few months we have seen a lot of Autonomous LLM agents that run on GPT-4. As we all know, GPT-4 is expensive, not everyone has access to it.\n\nSo I decided to play around with some open source LLM models that could run locally. I wanted to explore if we could create agents with these open source models and have them perform well...\n\nSo every time a new Open Source LLM model came out I tested it by adjusting my initial prompt. They all failed because models like Vicuna were good in imitating not reasoning (as highlighted by the ORCA paper).\n\nHowever, as some of you might have noticed, models trained for coding displayed some form of reasoning, at least that is what I noticed with StarCoder. Unfortunately, StarCoder was close but not good or consistent.\n\nToday, I have finally found our winner Wizcoder-15B (4-bit quantised). Here is a demo for you. In this demo, the agent trains RandomForest on Titanic dataset and saves the ROC Curve.\n\nYou can find the repo here: [https://github.com/Nuggt-dev/Nuggt](https://github.com/Nuggt-dev/Nuggt)\n\nWould love to hear your feedback!\n Comment: Can you test the new orca mini v2?\n Comment: What do yall think can be some useful applications for Nuggt?\n Comment: I tried myself to do something like this but prompting open models could sometimes be very hard. They are very sensitive to used words and sentences. Will look later how you got around it. I also saw that you are using only GPU inferencing and I think using also CPU one would expand potential users because not everybody has good graphics card available. Ctransformers on github can run star coder based models on CPU. It would be nice if you add that\n Comment: How can we use this to build our own GPU?\n Comment: Interesting work. I'm also working on an agent framework that allows the use of any model that's hosted via api. We have support for GPT, Claude, Oobabooga, and soon will be implementing Prem. I'd be interested to read more in-depth on the specific models you tested, what you found lacking, and methods you used to try to overcome those obstacles.\n Comment: Performance?\n Comment: Would love to use this but I need to know what privacy I can expect.  Is any data processing outside of my local rig?  Any data collection whatsoever ?\n Comment: Alright would test it tonight\n Comment: Sorry for the delay guys. I have tested orca mini v2 7b. I used two templates as follows:  \n\n\n1. Template 1: Thought/Action/Action Input/Observation \n2. Template 2: Step/Reason/Action/Action Input/Observation\n\nDefinition of Chunk: A chunk is simply one iteration of the template.\n\nThe test has three parts as follows:\n\n1. Testing if the model adheres to the template format. \n2. Testing if the model can come up with the entire correct solution (all chunks at the same time)\n3. Testing if the model can come up with the correct solution one chunk at a time. For this test the string \"Observation\" is set as end of token. After the model outputs one chunk, the Action/Action Input of that chunk is executed and the output of the Action is then appended to the model output as Observation. This is then passed again to the model for the next chunk.   \n\n\nFollowing are the results:\n\n1. Template 1: \n   1. Test 1: Pass\n   2. Test 2: Fail (The model does not generate the correct action input)\n   3. Test 3: Fail (The model did not output a correct solution)\n2. Template 2:\n   1. Test 1: Fail (The model did not have \"Action Input\" in the answer)\n   2. Test 2: Barely Pass (The model gave a complicated but correct answer)\n   3. Test 3: Fail (The model gave an incorrect answer)\n\n&#x200B;\n\nLet me know your thoughts on this and any feedback to change the tests or add additional tests (I am still working things out as well to come up with a standardised test). Sorry for bad news for Orca but hopefully with further iterations we can have better results.\n Comment: Translate old C++ code into rust. Maybe start with a small library, but the end goal would be to translate behemoths like opencv and llvm into rust.\n Comment: Digital assistant\n1) Show me all clients from my CRM who received a Quote but did not pay ?\n2) Send an email to these people with a survey requesting why they didn't move ahead\n Comment: Yes you are absolutely right, even the smallest of changes can sidetrack these models. Would love your feedback on my implementation. \n\nAs for CPU inference, expect a GGML PR soon! Would ping you here once i merge it\n Comment: build our own GPU? as in run on your local computer?\n Comment: I am coming up with a standardised test to measure performance. However, if you see my reply for orca in this thread, WizardCoder can pass all tests. \n\nLet me know if you have any ideas for performance test, would love to implement them\n Comment: Hi there, thank you for this impt question. \n\nMy code is completely open source so there is nothing to hide. The model is run by you either locally or on your server so the data stays with you. Lastly, there are some google sheet tracking codes that have been commented out (check the [nuggt.py](https://nuggt.py) file) i used those lines to keep track of model output but I have commented them in the github repo. You can uncomment it and add your own google sheet for testing purposes. \n\nAll in all I value privacy very much as you and thats why i created this. Rest assured your data stays with you. Let me know if you have any other questions\n\nAl\n Comment: !remindme 24 hours\n Comment: !remindme 12 hrs\n Comment: So? Have you tested it yet? How is it?\n Comment: What would be the advantage of rust over c++ ?\n Comment: Sounds great, I will create an agent with nuggt for this and let you know the results.\n Comment: This can be a very important use case for many agencies/companies out there. Will create an agent with nuggt for this and let you know the results. thanks!\n Comment: No I mean tape out open source GPU chip in verilog\n Comment: Oh ok thanks!\n Comment: I'm quite new to this game and struggling with the setup.\n\n1. Python complains about a missing \"run\" module (i'm running it in a separate tab instead)\n2. Is it not possible to run anything without API keys, even when hosting the model locally?\n Comment: I will be messaging you in 1 day on [**2023-07-06 05:39:18 UTC**](http://www.wolframalpha.com/input/?i=2023-07-06%2005:39:18%20UTC%20To%20Local%20Time) to remind you of [**this link**](https://www.reddit.com/r/singularity/comments/14r0x44/nuggt_a_llm_agent_the_runs_on_wizardcoder15b_4bit/jqq2mq2/?context=3)\n\n[**2 OTHERS CLICKED THIS LINK**](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=Reminder&message=%5Bhttps%3A%2F%2Fwww.reddit.com%2Fr%2Fsingularity%2Fcomments%2F14r0x44%2Fnuggt_a_llm_agent_the_runs_on_wizardcoder15b_4bit%2Fjqq2mq2%2F%5D%0A%0ARemindMe%21%202023-07-06%2005%3A39%3A18%20UTC) to send a PM to also be reminded and to reduce spam.\n\n^(Parent commenter can ) [^(delete this message to hide from others.)](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=Delete%20Comment&message=Delete%21%2014r0x44)\n\n*****\n\n|[^(Info)](https://www.reddit.com/r/RemindMeBot/comments/e1bko7/remindmebot_info_v21/)|[^(Custom)](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=Reminder&message=%5BLink%20or%20message%20inside%20square%20brackets%5D%0A%0ARemindMe%21%20Time%20period%20here)|[^(Your Reminders)](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=List%20Of%20Reminders&message=MyReminders%21)|[^(Feedback)](https://www.reddit.com/message/compose/?to=Watchful1&subject=RemindMeBot%20Feedback)|\n|-|-|-|-|\n Comment: Rust has an official package manager and build system, which means you'll never have to think about importing packages or building your code.\n\nRust is also memory safe without a garbage collector. Which means you don't have to think about accidentally creating memory bugs which are extremely frustrating to hunt down and fix.\n Comment: As someone who learned on c++ and recently picked up rust; rust is pretty functionally better in almost every way that has mattered to my use cases. It's a little obtuse syntacticly, but it has a lot of nice modern syntax structures I got used to in kotlin and c#. Forget the slick packaging system, it's just more flexible with what you can code and is still very efficient resource wise"
    },
    {
        "title": "[R] AIOS: LLM Agent Operating System",
        "link": "https://www.reddit.com/r/MachineLearning/comments/1booy6k/r_aios_llm_agent_operating_system/",
        "text": "**Paper**: [https://arxiv.org/abs/2403.16971](https://arxiv.org/abs/2403.16971)\n\n**Github**: [https://github.com/agiresearch/AIOS](https://github.com/agiresearch/AIOS)\n\n>**Abstract**: The integration and deployment of large language model (LLM)-based intelligent agents have been fraught with challenges that compromise their efficiency and efficacy. Among these issues are sub-optimal scheduling and resource allocation of agent requests over the LLM, the difficulties in maintaining context during interactions between agent and LLM, and the complexities inherent in integrating heterogeneous agents with different capabilities and specializations. The rapid increase of agent quantity and complexity further exacerbates these issues, often leading to bottlenecks and sub-optimal utilization of resources. Inspired by these challenges, this paper presents AIOS, an LLM agent operating system, which embeds large language model into operating systems (OS) as the brain of the OS, enabling an operating system \"with soul\" -- an important step towards AGI. Specifically, AIOS is designed to optimize resource allocation, facilitate context switch across agents, enable concurrent execution of agents, provide tool service for agents, and maintain access control for agents. We present the architecture of such an operating system, outline the core challenges it aims to resolve, and provide the basic design and implementation of the AIOS. Our experiments on concurrent execution of multiple agents demonstrate the reliability and efficiency of our AIOS modules. Through this, we aim to not only improve the performance and efficiency of LLM agents but also to pioneer for better development and deployment of the AIOS ecosystem in the future.\n\n[An overview of the AIOS architecture.](https://preview.redd.it/p7cxll6rzrqc1.png?width=1661&format=png&auto=webp&s=34fb90d183547ab45efd97c7df2ff8db7546d013)\n Comment: Just why? Seems like you're proposing giving an llm privileged access to kernel space which seems dangerous to me\n Comment: This is b**ls**t\n Comment: What is the difference between the first and second paper? And are they submitted to some conference or journal?\n\nIs this inspired from Karpathys tweet? [https://twitter.com/karpathy/status/1723140519554105733?lang=en](https://twitter.com/karpathy/status/1723140519554105733?lang=en) (It's nowhere referenced as far as I see. Although the tweet came a bit earlier than both of the papers, but probably concurrent ideas anyway.)\n\nThe same authors/group also did this: OpenAGI: When LLM Meets Domain Experts, published at NeurIPS 2023 (https://openreview.net/forum?id=gFf0a0ZxJM, https://arxiv.org/abs/2304.04370), repo: [https://github.com/agiresearch/OpenAGI](https://github.com/agiresearch/OpenAGI)\n\nAnd WarAgent? [https://github.com/agiresearch/WarAgent](https://github.com/agiresearch/WarAgent)\n Comment: Found [1 relevant code implementation](https://www.catalyzex.com/paper/arxiv:2403.16971/code) for \"AIOS: LLM Agent Operating System\".\n\nIf you have code to share with the community, please add it [here](https://www.catalyzex.com/add_code?paper_link=https://arxiv.org/abs/2403.16971&title=AIOS%3A+LLM+Agent+Operating+System) 😊🙏\n\n--\n\nTo opt out from receiving code links, DM me."
    },
    {
        "title": "🌟📚 Introducing LLM-Agents-Papers-for-Simulation Repository! 📚🌟",
        "link": "https://www.reddit.com/r/AutoGenAI/comments/1c8re4k/introducing_llmagentspapersforsimulation/",
        "text": " Hey everyone!\n\nI'm thrilled to announce the launch of a brand new repository on GitHub called [LLM-Agents-Papers-for-Simulation](https://github.com/giammy677dev/LLM-Agents-Papers-for-Simulation). As a recent graduate with a Master's in Computer Science, and now embarking on my doctoral journey focusing on this very topic, I'm passionate about bringing together a community interested in the intersection of simulation and LLMs.\n\n**What's this repository all about?**\n\nIn the ever-evolving landscape of understanding complex systems, simulation plays a crucial role. And with the advent of LLM-powered agents, we're witnessing a revolution in simulation methodologies. This repository serves as a central hub where we curate an extensive collection of resources showcasing how LLM technology intersects with simulation.\n\n**What can you find in this repository?**\n\nWe've got it all! From cutting-edge papers to insightful repositories, there's something for everyone!\n\n**How can you contribute?**\n\nWe're all about community collaboration! Whether you have papers, repositories, or resources to share, your contributions are invaluable. Simply submit pull requests or raise issues to help us keep this repository updated and relevant. Together, let's unlock new insights and pave the way for groundbreaking discoveries.\n\nThis repository isn't just about collecting resources; it's about fostering a vibrant community of researchers, enthusiasts, and practitioners passionate about simulation and LLM technology. Whether you're a seasoned expert or just dipping your toes into the field, there's a place for you here.\n\nLooking forward to seeing you there!\n Comment: Love the initiative! How's the AI integration part going?\n Comment: Very convenient. As somebody on the builder side, it is nice to be shoveled this information rather than needing to spend cycles to find it.\n\nI have a question: open source projects can fizzle out unless there is some alignment of interests for the contributing parties. Why is this project going to continue to be maintained?\n Comment: Currently, there are some tasks that are functioning quite well. The issue is understanding how to validate the simulations after running the LLM based on the output it produces.\n Comment: From what I've seen so far, open-source projects are the beating heart of simulations powered by LLM. Clearly, commercial models will always become cheaper over time compared to today, but currently, simulating a scenario involving hundreds of different agents all powered by LLM is very expensive when using a commercial model like ChatGPT. And that's where open-source models come into play (hardware permitting)."
    },
    {
        "title": "LLM Agents can Autonomously Exploit One-day Vulnerabilities",
        "link": "https://arxiv.org/pdf/2404.08144.pdf",
        "text": "\n Comment: From skimming this paper, it seems like a very interesting approach. The most wild bit is it took 91 lines of code to prototype! \n\nThey craft a prompt template (which they have with-held) and then give the agent a series of tools, one of which allows the agent to get more data via searching the web. The vulns are pretty web centric with a few XSS and sqli attacks but the authors do raise a good point that most are past GPT-4's knowledge cut off (i.e it couldn't have seen info about them before). \n\nI look forward to the authors dropping more code because, having experimented with and deployed a couple of LLM based projects, I am surprised no other LLM managed to achieve a single success. Whenever I have had really bad performance, it's usually down to the prompt formats needing to be different (mistral 7b instruct for examples has a different prompt template and required tags than OpenAI)\n\nThat being said, a very cool paper!"
    },
    {
        "title": "AIOS: LLM Agent Operating System",
        "link": "https://github.com/agiresearch/AIOS",
        "text": ""
    },
    {
        "title": "What's the use case for LLM agents?",
        "link": "https://www.reddit.com/r/LLMDevs/comments/1bevt1r/whats_the_use_case_for_llm_agents/",
        "text": "I see agent frameworks popping up all over the place. Per my understanding, all an agent is, is an LLM + functions. I pass a system prompt with a selection of functions, the agent uses the incoming user prompt to understand what functions to call with what payload.\n\nIt seems that it's easier to accomplish this by just using an LLM to transform structured data into structured data, and have regular software logic execute a function based on that structured data. It's easier to evaluate and track performance over time using evaluation datasets as well as monitor regression. It's much harder for me to guarantee that given an input, X actions will happen in an agent setting vs a regular LLM.\n\nAm I missing something? Are there workflows you can achieve with agents that you cannot without?\n Comment: Your question has the answer :)\n\n> It seems that it's easier to accomplish this by just using an LLM to transform structured data into structured data, and have regular software logic execute a function based on that structured data. It's easier to evaluate and track performance over time using evaluation datasets as well as monitor regression\n\nFrameworks don't do any magic. What a simple web framework does can be accomplished by writing code. But since most of the people have this basic requirement (converting response to unstructured, abstract interfaces to easily switch LLMs, ways to evaluate response etc), we have framework to make you start quickly without reinventing the wheel.\n\nBut just like other boilerplate it depends on framework whether it empowers you and gives flexibility or restricts you in doing things only in a certain way. \n\nJust like web evolved and became the basic requirement, with advancement in LLM, they are becoming very common in apps and sites so it makes sense to provide framework to simplify most of the complex operations"
    },
    {
        "title": "🌟📚 Introducing LLM-Agents-Papers-for-Simulation Repository! 📚🌟",
        "link": "https://www.reddit.com/r/LocalLLaMA/comments/1c8rdmm/introducing_llmagentspapersforsimulation/",
        "text": " Hey everyone!\n\nI'm thrilled to announce the launch of a brand new repository on GitHub called [LLM-Agents-Papers-for-Simulation](https://github.com/giammy677dev/LLM-Agents-Papers-for-Simulation). As a recent graduate with a Master's in Computer Science, and now embarking on my doctoral journey focusing on this very topic, I'm passionate about bringing together a community interested in the intersection of simulation and LLMs.\n\n**What's this repository all about?**\n\nIn the ever-evolving landscape of understanding complex systems, simulation plays a crucial role. And with the advent of LLM-powered agents, we're witnessing a revolution in simulation methodologies. This repository serves as a central hub where we curate an extensive collection of resources showcasing how LLM technology intersects with simulation.\n\n**What can you find in this repository?**\n\nWe've got it all! From cutting-edge papers to insightful repositories, there's something for everyone!\n\n**How can you contribute?**\n\nWe're all about community collaboration! Whether you have papers, repositories, or resources to share, your contributions are invaluable. Simply submit pull requests or raise issues to help us keep this repository updated and relevant. Together, let's unlock new insights and pave the way for groundbreaking discoveries.\n\nThis repository isn't just about collecting resources; it's about fostering a vibrant community of researchers, enthusiasts, and practitioners passionate about simulation and LLM technology. Whether you're a seasoned expert or just dipping your toes into the field, there's a place for you here.\n\nLooking forward to seeing you there!"
    },
    {
        "title": "LLM Agents can Autonomously Hack Websites",
        "link": "https://arxiv.org/html/2402.06664v1",
        "text": "\n Comment: Really cool research. I feel like a more co-pilot like approach would be absolutely amazing. I imagine it would try to execute a lot of commands that simply doesn’t work, but just needs a bit of tweaking by human. And it can suggest creative avenues for manual testing."
    },
    {
        "title": "LLM Agents can Autonomously Hack Websites",
        "link": "https://arxiv.org/abs/2402.06664",
        "text": ""
    },
    {
        "title": "Looking for best resources on LLM Agents",
        "link": "https://www.reddit.com/r/learnmachinelearning/comments/1bifeu2/looking_for_best_resources_on_llm_agents/",
        "text": "I am looking for best resources which can guide me to train LLM agent better."
    },
    {
        "title": "LLM agent writes code to train robots",
        "link": "https://www.reddit.com/r/martechnewser/comments/1clg5jq/llm_agent_writes_code_to_train_robots/",
        "text": "Nvidia researchers just **introduced** DrEureka, an AI system that uses LLM agents to automate the process of training robot skills in simulations and transferring them to the real world.\n\n**Stay ahead of the curve with the latest trends in tech and marketing – join our subreddit community** r/martechnewser **today for instant notifications!**\n\nhttps://preview.redd.it/q58n8lioesyc1.jpg?width=1292&format=pjpg&auto=webp&s=48da1bbd3f7b6727063d307d63553dbde9811cd7\n\n**The details:**\n\n* DrEureka writes code to teach robot skills in simulations, then ensures skills work well when transferred to physical robots.\n* The system trained a simulated robot dog to balance and walk on a yoga ball, with the task then transferred successfully to a real robot in testing.\n* DrEureka uses LLMs’ physics knowledge to adjust parameters like friction and gravity, automating the time-consuming process of adapting to the real world.\n\n**Why it matters:** Enabling AI to oversee and improve the robot learning process will massively accelerate advanced robotic deployments, while also reducing the need for human experts in the loop. DrEureka allows robots to become much more capable learners — tackling difficult tasks in a fraction of the time.\n Comment: 🤖 Nvidia’s DrEureka can automate robot training\n\nIn robotics, one of the biggest challenges is transferring skills learned in **simulation to real-world environments**. NVIDIA recently revealed **DrEureka**.\n\nIt automates the entire *sim-to-real* process for robots. It allows them to learn new skills in simulated environments and deploy them in the real world.\n\n**How does it work?**\n\nDrEureka builds on NVIDIA’s previous project, Eureka, which trained a robot hand to spin a pen. **It’s a more advanced AI** that writes its own codes to train robots in simulations and then transfers these skills to real-world tasks automatically.\n\n**This automation is significant** because it eliminates the need for experts to manually adjust each parameter, such as gravity and friction. DrEureka does all this on its own and *even explains the changes it makes*."
    }
]